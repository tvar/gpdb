<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE topic PUBLIC "-//OASIS//DTD DITA Topic//EN" "topic.dtd">
<topic id="topic_gptransfer">
  <title>Migrating Data with gptransfer</title>
  <shortdesc>Information about the <codeph>gptransfer</codeph> migration utility, which transfers
    Greenplum Database metadata and data from one Greenplum database to another.</shortdesc>
  <body>
    <!-- conref'ing to content from admin_guide. -->
    <note conref="../admin_guide/managing/gptransfer.xml#note1"/>
    <p conref="../admin_guide/managing/gptransfer.xml#intro1"/>
    <p conref="../admin_guide/managing/gptransfer.xml#intro2"/>
    <p conref="../admin_guide/managing/gptransfer.xml#intro3"/>
    <p conref="../admin_guide/managing/gptransfer.xml#intro4"/>

    <section conref="../admin_guide/managing/gptransfer.xml#prereqs"/>
    <section conref="../admin_guide/managing/gptransfer.xml#whatgptdoes"/>
    <section conref="../admin_guide/managing/gptransfer.xml#fastslow"/>
    <section conref="../admin_guide/managing/gptransfer.xml#batch"/>
    <section conref="../admin_guide/managing/gptransfer.xml#prephosts"/>
    <section conref="../admin_guide/managing/gptransfer.xml#limitations"/>
    <section conref="../admin_guide/managing/gptransfer.xml#fulltblmode"/>
    <section conref="../admin_guide/managing/gptransfer.xml#locking"/>
    <section conref="../admin_guide/managing/gptransfer.xml#validation"/>
    <section conref="../admin_guide/managing/gptransfer.xml#failedtran"/>

    <section>
      <title>Best Practices</title>
      <p><codeph>gptransfer</codeph> creates a configuration that allows transferring large amounts
        of data at very high rates. For small or empty tables, however, the
          <codeph>gptransfer</codeph> set up and clean up are too expensive. The best practice is to
        use <codeph>gptransfer</codeph> for large tables and to use other methods for copying
        smaller tables. </p>
      <ol>
        <li>Before you begin to transfer data, replicate the schema or schemas from the source
          cluster to the destination cluster. Options for copying the
            schema include:<ul id="ul_q1y_1hh_1s">
            <li>Use the PostgreSQL <codeph>pg_dump</codeph> or <codeph>pg_dumpall</codeph> utility
              with the <codeph>â€“-schema-only</codeph> option.</li>
            <li>DDL scripts, or any other method for recreating schema in the destination
              database.</li>
          </ul>
        </li>
        <li>Divide the non-empty tables to be transferred into large and small categories, using
          criteria of your own choice. For example, you could decide large tables have more than one
          million rows or a raw data size greater than 1GB. </li>
        <li>Transfer data for small tables using the SQL <codeph>COPY</codeph> command. This
          eliminates the warm-up/cool-down time each table incurs when using the
            <codeph>gptransfer</codeph> utility.<ul id="ul_ih2_dqm_1s">
            <li>Optionally, write or reuse existing shell scripts to loop through a list of table
              names to copy with the <codeph>COPY</codeph> command.</li>
          </ul></li>
        <li>Use <codeph>gptransfer</codeph> to transfer the large table data in batches of
            tables.<ul id="ul_h1z_pqm_1s">
            <li>It is best to transfer to the same size cluster or to a larger cluster so that
                <codeph>gptransfer</codeph> runs in fast mode. </li>
            <li>If any indexes exist, drop them before starting the transfer process.</li>
            <li>Use the <codeph>gptransfer</codeph> table (<codeph>-t</codeph>) or file
                (<codeph>-f</codeph>) options to execute the migration in batches of tables. Do not
              run <codeph>gptransfer</codeph> using the full mode; the schema and smaller tables
              have already been transferred.</li>
            <li>Perform test runs of the <codeph>gptransfer</codeph> process before the production
              migration. This ensures tables can be transferred successfully. You can experiment
              with the <codeph>--batch-size</codeph> and <codeph>--sub-batch-size</codeph> options
              to obtain maximum parallelism. Determine proper batching of tables for iterative
                <codeph>gptransfer</codeph> runs.</li>
            <li>Include the <codeph>--skip-existing</codeph> option because the schema already
              exists on the destination cluster.</li>
            <li>Use only fully qualified table names. Be aware that periods (.), whitespace, quotes
              (') and double quotes (") in table names may cause problems. </li>
            <li>If you decide to use the <codeph>--validation</codeph> option to validate the data
              after transfer, be sure to also use the <codeph>-x</codeph> option to place an
              exclusive lock on the source table.</li>
          </ul>
        </li>
        <li>After all tables are transferred, perform the following tasks:<ul id="ul_dgz_f5m_1s">
            <li>Check for and correct any failed transfers.</li>
            <li>Recreate the indexes that were dropped before the transfer. </li>
            <li>Ensure any roles, functions, and resource queues are created in the destination
              database. These objects are not transferred when you use the <codeph>gptransfer
                -t</codeph> option. </li>
            <li>Copy the <codeph>postgresql.conf</codeph> and <codeph>pg_hba.conf</codeph>
              configuration files from the source to the destination cluster. </li>
            <li>Install needed extensions in the destination database with
              <codeph>gppkg</codeph>.</li>
          </ul>
        </li>
      </ol>
    </section>
  </body>
</topic>
